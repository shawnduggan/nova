/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// main.ts
var main_exports = {};
__export(main_exports, {
  default: () => NovaPlugin
});
module.exports = __toCommonJS(main_exports);
var import_obsidian4 = require("obsidian");

// src/settings.ts
var import_obsidian = require("obsidian");
var DEFAULT_SETTINGS = {
  aiProviders: {
    claude: {
      apiKey: "",
      model: "claude-sonnet-4",
      temperature: 0.7,
      maxTokens: 1e3
    },
    openai: {
      apiKey: "",
      baseUrl: "https://api.openai.com/v1",
      model: "gpt-4o",
      temperature: 0.7,
      maxTokens: 1e3
    },
    google: {
      apiKey: "",
      model: "gemini-2.5-flash",
      temperature: 0.7,
      maxTokens: 1e3
    },
    ollama: {
      baseUrl: "http://localhost:11434",
      model: "",
      temperature: 0.7,
      maxTokens: 1e3
    }
  },
  platformSettings: {
    desktop: {
      primaryProvider: "ollama",
      fallbackProviders: ["openai", "google", "ollama"]
    },
    mobile: {
      primaryProvider: "none",
      fallbackProviders: ["openai", "google"]
    }
  },
  general: {
    defaultTemperature: 0.7,
    defaultMaxTokens: 1e3,
    autoSave: true
  }
};
var NovaSettingTab = class extends import_obsidian.PluginSettingTab {
  constructor(app, plugin) {
    super(app, plugin);
    this.plugin = plugin;
  }
  display() {
    const { containerEl } = this;
    containerEl.empty();
    containerEl.createEl("h2", { text: "Nova AI Settings" });
    this.createGeneralSettings();
    this.createPlatformSettings();
    this.createProviderSettings();
  }
  createGeneralSettings() {
    const { containerEl } = this;
    containerEl.createEl("h3", { text: "General Settings" });
    new import_obsidian.Setting(containerEl).setName("Default Temperature").setDesc("Controls randomness in AI responses (0.0 - 1.0)").addSlider((slider) => slider.setLimits(0, 1, 0.1).setValue(this.plugin.settings.general.defaultTemperature).setDynamicTooltip().onChange(async (value) => {
      this.plugin.settings.general.defaultTemperature = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(containerEl).setName("Default Max Tokens").setDesc("Maximum length of AI responses").addText((text) => text.setPlaceholder("1000").setValue(this.plugin.settings.general.defaultMaxTokens.toString()).onChange(async (value) => {
      const numValue = parseInt(value);
      if (!isNaN(numValue) && numValue > 0) {
        this.plugin.settings.general.defaultMaxTokens = numValue;
        await this.plugin.saveSettings();
      }
    }));
    new import_obsidian.Setting(containerEl).setName("Auto-save settings").setDesc("Automatically save settings when changed").addToggle((toggle) => toggle.setValue(this.plugin.settings.general.autoSave).onChange(async (value) => {
      this.plugin.settings.general.autoSave = value;
      await this.plugin.saveSettings();
    }));
  }
  createProviderSettings() {
    const { containerEl } = this;
    containerEl.createEl("h3", { text: "AI Provider Settings" });
    this.createOllamaSettings();
    this.createClaudeSettings();
    this.createGoogleSettings();
    this.createOpenAISettings();
  }
  createClaudeSettings() {
    const { containerEl } = this;
    const claudeContainer = containerEl.createDiv({ cls: "nova-provider-section" });
    claudeContainer.createEl("h4", { text: "Claude (Anthropic)" });
    new import_obsidian.Setting(claudeContainer).setName("API Key").setDesc("Your Anthropic API key").addText((text) => text.setPlaceholder("sk-ant-...").setValue(this.plugin.settings.aiProviders.claude.apiKey || "").onChange(async (value) => {
      this.plugin.settings.aiProviders.claude.apiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(claudeContainer).setName("Model").setDesc("Claude model to use").addDropdown((dropdown) => dropdown.addOption("claude-sonnet-4", "Claude Sonnet 4").addOption("claude-opus-4", "Claude Opus 4").addOption("claude-3-haiku-20240307", "Claude 3 Haiku").addOption("claude-3-sonnet-20240229", "Claude 3 Sonnet").addOption("claude-3-opus-20240229", "Claude 3 Opus").setValue(this.plugin.settings.aiProviders.claude.model || "claude-sonnet-4").onChange(async (value) => {
      this.plugin.settings.aiProviders.claude.model = value;
      await this.plugin.saveSettings();
    }));
  }
  createOpenAISettings() {
    const { containerEl } = this;
    const openaiContainer = containerEl.createDiv({ cls: "nova-provider-section" });
    openaiContainer.createEl("h4", { text: "OpenAI" });
    new import_obsidian.Setting(openaiContainer).setName("API Key").setDesc("Your OpenAI API key").addText((text) => text.setPlaceholder("sk-...").setValue(this.plugin.settings.aiProviders.openai.apiKey || "").onChange(async (value) => {
      this.plugin.settings.aiProviders.openai.apiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(openaiContainer).setName("Base URL").setDesc("OpenAI API base URL (for custom endpoints)").addText((text) => text.setPlaceholder("https://api.openai.com/v1").setValue(this.plugin.settings.aiProviders.openai.baseUrl || "").onChange(async (value) => {
      this.plugin.settings.aiProviders.openai.baseUrl = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(openaiContainer).setName("Model").setDesc("OpenAI model to use").addDropdown((dropdown) => dropdown.addOption("gpt-4o", "GPT-4o").addOption("gpt-3.5-turbo", "GPT-3.5 Turbo").addOption("gpt-4", "GPT-4").addOption("gpt-4-turbo-preview", "GPT-4 Turbo").setValue(this.plugin.settings.aiProviders.openai.model || "gpt-4o").onChange(async (value) => {
      this.plugin.settings.aiProviders.openai.model = value;
      await this.plugin.saveSettings();
    }));
  }
  createGoogleSettings() {
    const { containerEl } = this;
    const googleContainer = containerEl.createDiv({ cls: "nova-provider-section" });
    googleContainer.createEl("h4", { text: "Google (Gemini)" });
    new import_obsidian.Setting(googleContainer).setName("API Key").setDesc("Your Google AI API key").addText((text) => text.setPlaceholder("AI...").setValue(this.plugin.settings.aiProviders.google.apiKey || "").onChange(async (value) => {
      this.plugin.settings.aiProviders.google.apiKey = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(googleContainer).setName("Model").setDesc("Gemini model to use").addDropdown((dropdown) => dropdown.addOption("gemini-2.5-flash", "Gemini 2.5 Flash").addOption("gemini-2.5-pro", "Gemini 2.5 Pro").addOption("gemini-pro", "Gemini Pro").addOption("gemini-pro-vision", "Gemini Pro Vision").setValue(this.plugin.settings.aiProviders.google.model || "gemini-2.5-flash").onChange(async (value) => {
      this.plugin.settings.aiProviders.google.model = value;
      await this.plugin.saveSettings();
    }));
  }
  createOllamaSettings() {
    const { containerEl } = this;
    const ollamaContainer = containerEl.createDiv({ cls: "nova-provider-section" });
    ollamaContainer.createEl("h4", { text: "Ollama (Local)" });
    new import_obsidian.Setting(ollamaContainer).setName("Base URL").setDesc("Ollama server URL").addText((text) => text.setPlaceholder("http://localhost:11434").setValue(this.plugin.settings.aiProviders.ollama.baseUrl || "").onChange(async (value) => {
      this.plugin.settings.aiProviders.ollama.baseUrl = value;
      await this.plugin.saveSettings();
    }));
    new import_obsidian.Setting(ollamaContainer).setName("Model").setDesc("Ollama model to use").addText((text) => text.setPlaceholder("llama2").setValue(this.plugin.settings.aiProviders.ollama.model || "").onChange(async (value) => {
      this.plugin.settings.aiProviders.ollama.model = value;
      await this.plugin.saveSettings();
    }));
  }
  createPlatformSettings() {
    const { containerEl } = this;
    containerEl.createEl("h3", { text: "Platform Settings" });
    const platformContainer = containerEl.createDiv({ cls: "nova-platform-section" });
    platformContainer.createEl("h4", { text: "Desktop" });
    new import_obsidian.Setting(platformContainer).setName("Primary Provider").setDesc("Primary AI provider for desktop").addDropdown((dropdown) => dropdown.addOption("claude", "Claude").addOption("openai", "OpenAI").addOption("google", "Google").addOption("ollama", "Ollama").setValue(this.plugin.settings.platformSettings.desktop.primaryProvider).onChange(async (value) => {
      this.plugin.settings.platformSettings.desktop.primaryProvider = value;
      await this.plugin.saveSettings();
    }));
    platformContainer.createEl("h4", { text: "Mobile" });
    new import_obsidian.Setting(platformContainer).setName("Primary Provider").setDesc("Primary AI provider for mobile").addDropdown((dropdown) => dropdown.addOption("none", "None (Disabled)").addOption("claude", "Claude").addOption("openai", "OpenAI").addOption("google", "Google").setValue(this.plugin.settings.platformSettings.mobile.primaryProvider).onChange(async (value) => {
      this.plugin.settings.platformSettings.mobile.primaryProvider = value;
      await this.plugin.saveSettings();
    }));
  }
};

// src/ai/provider-manager.ts
var import_obsidian2 = require("obsidian");

// src/ai/providers/claude.ts
var ClaudeProvider = class {
  constructor(config) {
    this.name = "Claude (Anthropic)";
    this.config = config;
  }
  updateConfig(config) {
    this.config = config;
  }
  async isAvailable() {
    return !!this.config.apiKey;
  }
  async generateText(prompt, options) {
    const messages = [{ role: "user", content: prompt }];
    return this.chatCompletion(messages, options);
  }
  async *generateTextStream(prompt, options) {
    const messages = [{ role: "user", content: prompt }];
    yield* this.chatCompletionStream(messages, options);
  }
  async chatCompletion(messages, options) {
    if (!this.config.apiKey) {
      throw new Error("Claude API key not configured");
    }
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "x-api-key": this.config.apiKey,
        "anthropic-version": "2023-06-01"
      },
      body: JSON.stringify({
        model: (options == null ? void 0 : options.model) || this.config.model || "claude-3-haiku-20240307",
        max_tokens: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3,
        temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
        system: options == null ? void 0 : options.systemPrompt,
        messages: messages.map((msg) => ({
          role: msg.role === "assistant" ? "assistant" : "user",
          content: msg.content
        }))
      })
    });
    if (!response.ok) {
      throw new Error(`Claude API error: ${response.statusText}`);
    }
    const data = await response.json();
    return data.content[0].text;
  }
  async *chatCompletionStream(messages, options) {
    var _a, _b;
    if (!this.config.apiKey) {
      throw new Error("Claude API key not configured");
    }
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "x-api-key": this.config.apiKey,
        "anthropic-version": "2023-06-01"
      },
      body: JSON.stringify({
        model: (options == null ? void 0 : options.model) || this.config.model || "claude-3-haiku-20240307",
        max_tokens: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3,
        temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
        system: options == null ? void 0 : options.systemPrompt,
        stream: true,
        messages: messages.map((msg) => ({
          role: msg.role === "assistant" ? "assistant" : "user",
          content: msg.content
        }))
      })
    });
    if (!response.ok) {
      throw new Error(`Claude API error: ${response.statusText}`);
    }
    const reader = (_a = response.body) == null ? void 0 : _a.getReader();
    if (!reader) {
      throw new Error("Failed to get response reader");
    }
    const decoder = new TextDecoder();
    let buffer = "";
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.startsWith("data: ")) {
            const data = line.slice(6);
            if (data === "[DONE]") {
              yield { content: "", done: true };
              return;
            }
            try {
              const parsed = JSON.parse(data);
              if (parsed.type === "content_block_delta" && ((_b = parsed.delta) == null ? void 0 : _b.text)) {
                yield { content: parsed.delta.text, done: false };
              }
            } catch (e) {
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
    yield { content: "", done: true };
  }
};

// src/ai/providers/openai.ts
var OpenAIProvider = class {
  constructor(config) {
    this.name = "OpenAI";
    this.config = config;
  }
  updateConfig(config) {
    this.config = config;
  }
  async isAvailable() {
    return !!this.config.apiKey;
  }
  async generateText(prompt, options) {
    const messages = [{ role: "user", content: prompt }];
    return this.chatCompletion(messages, options);
  }
  async *generateTextStream(prompt, options) {
    const messages = [{ role: "user", content: prompt }];
    yield* this.chatCompletionStream(messages, options);
  }
  async chatCompletion(messages, options) {
    if (!this.config.apiKey) {
      throw new Error("OpenAI API key not configured");
    }
    const requestMessages = [...messages];
    if (options == null ? void 0 : options.systemPrompt) {
      requestMessages.unshift({ role: "system", content: options.systemPrompt });
    }
    const response = await fetch(this.config.baseUrl || "https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${this.config.apiKey}`
      },
      body: JSON.stringify({
        model: (options == null ? void 0 : options.model) || this.config.model || "gpt-3.5-turbo",
        messages: requestMessages,
        max_tokens: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3,
        temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7
      })
    });
    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.statusText}`);
    }
    const data = await response.json();
    return data.choices[0].message.content;
  }
  async *chatCompletionStream(messages, options) {
    var _a, _b, _c, _d;
    if (!this.config.apiKey) {
      throw new Error("OpenAI API key not configured");
    }
    const requestMessages = [...messages];
    if (options == null ? void 0 : options.systemPrompt) {
      requestMessages.unshift({ role: "system", content: options.systemPrompt });
    }
    const response = await fetch(this.config.baseUrl || "https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${this.config.apiKey}`
      },
      body: JSON.stringify({
        model: (options == null ? void 0 : options.model) || this.config.model || "gpt-3.5-turbo",
        messages: requestMessages,
        max_tokens: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3,
        temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
        stream: true
      })
    });
    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.statusText}`);
    }
    const reader = (_a = response.body) == null ? void 0 : _a.getReader();
    if (!reader) {
      throw new Error("Failed to get response reader");
    }
    const decoder = new TextDecoder();
    let buffer = "";
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.startsWith("data: ")) {
            const data = line.slice(6);
            if (data === "[DONE]") {
              yield { content: "", done: true };
              return;
            }
            try {
              const parsed = JSON.parse(data);
              const content = (_d = (_c = (_b = parsed.choices) == null ? void 0 : _b[0]) == null ? void 0 : _c.delta) == null ? void 0 : _d.content;
              if (content) {
                yield { content, done: false };
              }
            } catch (e) {
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
    yield { content: "", done: true };
  }
};

// src/ai/providers/google.ts
var GoogleProvider = class {
  constructor(config) {
    this.name = "Google (Gemini)";
    this.config = config;
  }
  updateConfig(config) {
    this.config = config;
  }
  async isAvailable() {
    return !!this.config.apiKey;
  }
  async generateText(prompt, options) {
    const messages = [{ role: "user", content: prompt }];
    return this.chatCompletion(messages, options);
  }
  async *generateTextStream(prompt, options) {
    const messages = [{ role: "user", content: prompt }];
    yield* this.chatCompletionStream(messages, options);
  }
  formatMessagesForGemini(messages, systemPrompt) {
    const contents = [];
    if (systemPrompt) {
      contents.push({
        role: "user",
        parts: [{ text: `System: ${systemPrompt}` }]
      });
    }
    for (const message of messages) {
      const role = message.role === "assistant" ? "model" : "user";
      contents.push({
        role,
        parts: [{ text: message.content }]
      });
    }
    return contents;
  }
  async chatCompletion(messages, options) {
    if (!this.config.apiKey) {
      throw new Error("Google API key not configured");
    }
    const model = (options == null ? void 0 : options.model) || this.config.model || "gemini-pro";
    const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${this.config.apiKey}`;
    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        contents: this.formatMessagesForGemini(messages, options == null ? void 0 : options.systemPrompt),
        generationConfig: {
          temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
          maxOutputTokens: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3
        }
      })
    });
    if (!response.ok) {
      throw new Error(`Google API error: ${response.statusText}`);
    }
    const data = await response.json();
    return data.candidates[0].content.parts[0].text;
  }
  async *chatCompletionStream(messages, options) {
    var _a, _b, _c, _d, _e, _f;
    if (!this.config.apiKey) {
      throw new Error("Google API key not configured");
    }
    const model = (options == null ? void 0 : options.model) || this.config.model || "gemini-pro";
    const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:streamGenerateContent?key=${this.config.apiKey}`;
    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Content-Type": "application/json"
      },
      body: JSON.stringify({
        contents: this.formatMessagesForGemini(messages, options == null ? void 0 : options.systemPrompt),
        generationConfig: {
          temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
          maxOutputTokens: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3
        }
      })
    });
    if (!response.ok) {
      throw new Error(`Google API error: ${response.statusText}`);
    }
    const reader = (_a = response.body) == null ? void 0 : _a.getReader();
    if (!reader) {
      throw new Error("Failed to get response reader");
    }
    const decoder = new TextDecoder();
    let buffer = "";
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split("\n");
        buffer = lines.pop() || "";
        for (const line of lines) {
          if (line.trim()) {
            try {
              const parsed = JSON.parse(line);
              const text = (_f = (_e = (_d = (_c = (_b = parsed.candidates) == null ? void 0 : _b[0]) == null ? void 0 : _c.content) == null ? void 0 : _d.parts) == null ? void 0 : _e[0]) == null ? void 0 : _f.text;
              if (text) {
                yield { content: text, done: false };
              }
            } catch (e) {
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
    yield { content: "", done: true };
  }
};

// src/ai/providers/ollama.ts
var OllamaProvider = class {
  constructor(config) {
    this.name = "Ollama (Local)";
    this.config = config;
  }
  updateConfig(config) {
    this.config = config;
  }
  async isAvailable() {
    if (!this.config.model) return false;
    try {
      const baseUrl = this.config.baseUrl || "http://localhost:11434";
      const response = await fetch(`${baseUrl}/api/tags`, {
        method: "GET",
        headers: { "Content-Type": "application/json" }
      });
      return response.ok;
    } catch (e) {
      return false;
    }
  }
  async generateText(prompt, options) {
    const baseUrl = this.config.baseUrl || "http://localhost:11434";
    const model = (options == null ? void 0 : options.model) || this.config.model;
    if (!model) {
      throw new Error("Ollama model must be specified");
    }
    const response = await fetch(`${baseUrl}/api/generate`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model,
        prompt,
        stream: false,
        options: {
          temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
          num_predict: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3
        }
      })
    });
    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`);
    }
    const data = await response.json();
    return data.response;
  }
  async *generateTextStream(prompt, options) {
    var _a;
    const baseUrl = this.config.baseUrl || "http://localhost:11434";
    const model = (options == null ? void 0 : options.model) || this.config.model;
    if (!model) {
      throw new Error("Ollama model must be specified");
    }
    const response = await fetch(`${baseUrl}/api/generate`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model,
        prompt,
        stream: true,
        options: {
          temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
          num_predict: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3
        }
      })
    });
    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`);
    }
    const reader = (_a = response.body) == null ? void 0 : _a.getReader();
    if (!reader) {
      throw new Error("Failed to get response reader");
    }
    const decoder = new TextDecoder();
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        const lines = decoder.decode(value).split("\n");
        for (const line of lines) {
          if (line.trim()) {
            try {
              const parsed = JSON.parse(line);
              if (parsed.response) {
                yield { content: parsed.response, done: false };
              }
              if (parsed.done) {
                yield { content: "", done: true };
                return;
              }
            } catch (e) {
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
    yield { content: "", done: true };
  }
  async chatCompletion(messages, options) {
    const baseUrl = this.config.baseUrl || "http://localhost:11434";
    const model = (options == null ? void 0 : options.model) || this.config.model;
    if (!model) {
      throw new Error("Ollama model must be specified");
    }
    const response = await fetch(`${baseUrl}/api/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model,
        messages: messages.map((msg) => ({
          role: msg.role,
          content: msg.content
        })),
        stream: false,
        options: {
          temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
          num_predict: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3
        }
      })
    });
    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`);
    }
    const data = await response.json();
    return data.message.content;
  }
  async *chatCompletionStream(messages, options) {
    var _a, _b;
    const baseUrl = this.config.baseUrl || "http://localhost:11434";
    const model = (options == null ? void 0 : options.model) || this.config.model;
    if (!model) {
      throw new Error("Ollama model must be specified");
    }
    const response = await fetch(`${baseUrl}/api/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model,
        messages: messages.map((msg) => ({
          role: msg.role,
          content: msg.content
        })),
        stream: true,
        options: {
          temperature: (options == null ? void 0 : options.temperature) || this.config.temperature || 0.7,
          num_predict: (options == null ? void 0 : options.maxTokens) || this.config.maxTokens || 1e3
        }
      })
    });
    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`);
    }
    const reader = (_a = response.body) == null ? void 0 : _a.getReader();
    if (!reader) {
      throw new Error("Failed to get response reader");
    }
    const decoder = new TextDecoder();
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        const lines = decoder.decode(value).split("\n");
        for (const line of lines) {
          if (line.trim()) {
            try {
              const parsed = JSON.parse(line);
              if ((_b = parsed.message) == null ? void 0 : _b.content) {
                yield { content: parsed.message.content, done: false };
              }
              if (parsed.done) {
                yield { content: "", done: true };
                return;
              }
            } catch (e) {
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }
    yield { content: "", done: true };
  }
};

// src/ai/provider-manager.ts
var AIProviderManager = class {
  constructor(settings) {
    this.providers = /* @__PURE__ */ new Map();
    this.settings = settings;
  }
  async initialize() {
    this.providers.set("claude", new ClaudeProvider(this.settings.aiProviders.claude));
    this.providers.set("openai", new OpenAIProvider(this.settings.aiProviders.openai));
    this.providers.set("google", new GoogleProvider(this.settings.aiProviders.google));
    this.providers.set("ollama", new OllamaProvider(this.settings.aiProviders.ollama));
  }
  updateSettings(settings) {
    this.settings = settings;
    this.providers.forEach((provider, type) => {
      var _a;
      if (type !== "none" && type in this.settings.aiProviders) {
        (_a = provider.updateConfig) == null ? void 0 : _a.call(provider, this.settings.aiProviders[type]);
      }
    });
  }
  getPlatformProviders() {
    const platform = import_obsidian2.Platform.isMobile ? "mobile" : "desktop";
    const platformSettings = this.settings.platformSettings[platform];
    return [platformSettings.primaryProvider, ...platformSettings.fallbackProviders];
  }
  async getAvailableProvider() {
    const orderedProviders = this.getPlatformProviders();
    if (orderedProviders[0] === "none") {
      return null;
    }
    for (const providerType of orderedProviders) {
      if (providerType === "none") continue;
      const provider = this.providers.get(providerType);
      if (provider && await provider.isAvailable()) {
        return provider;
      }
    }
    return null;
  }
  async generateText(prompt, options) {
    const provider = await this.getAvailableProvider();
    if (!provider) {
      throw new Error("Nova is disabled or no AI provider is available");
    }
    return provider.generateText(prompt, options);
  }
  async *generateTextStream(prompt, options) {
    const provider = await this.getAvailableProvider();
    if (!provider) {
      throw new Error("Nova is disabled or no AI provider is available");
    }
    yield* provider.generateTextStream(prompt, options);
  }
  async chatCompletion(messages, options) {
    const provider = await this.getAvailableProvider();
    if (!provider) {
      throw new Error("Nova is disabled or no AI provider is available");
    }
    return provider.chatCompletion(messages, options);
  }
  async *chatCompletionStream(messages, options) {
    const provider = await this.getAvailableProvider();
    if (!provider) {
      throw new Error("Nova is disabled or no AI provider is available");
    }
    yield* provider.chatCompletionStream(messages, options);
  }
  getProviderNames() {
    return Array.from(this.providers.values()).map((p) => p.name);
  }
  cleanup() {
    this.providers.clear();
  }
};

// src/ui/sidebar-view.ts
var import_obsidian3 = require("obsidian");
var VIEW_TYPE_NOVA_SIDEBAR = "nova-sidebar";
var NovaSidebarView = class extends import_obsidian3.ItemView {
  constructor(leaf, plugin) {
    super(leaf);
    this.plugin = plugin;
  }
  getViewType() {
    return VIEW_TYPE_NOVA_SIDEBAR;
  }
  getDisplayText() {
    return "Nova AI";
  }
  async onOpen() {
    const container = this.containerEl.children[1];
    container.empty();
    container.createEl("h4", { text: "Nova - Your AI Thinking Partner" });
    this.createChatInterface(container);
    this.createInputInterface(container);
  }
  async onClose() {
  }
  createChatInterface(container) {
    this.chatContainer = container.createDiv({ cls: "nova-chat-container" });
    this.chatContainer.style.cssText = `
			height: 60vh;
			overflow-y: auto;
			border: 1px solid var(--background-modifier-border);
			border-radius: 4px;
			padding: 10px;
			margin-bottom: 10px;
			background: var(--background-secondary);
		`;
    this.addMessage("assistant", "Hello! I'm Nova, your AI Thinking Partner. How can I assist you today?");
  }
  createInputInterface(container) {
    this.inputContainer = container.createDiv({ cls: "nova-input-container" });
    this.inputContainer.style.cssText = `
			display: flex;
			flex-direction: column;
			gap: 10px;
		`;
    const textAreaContainer = this.inputContainer.createDiv();
    this.textArea = new import_obsidian3.TextAreaComponent(textAreaContainer);
    this.textArea.setPlaceholder("Ask Nova anything...");
    this.textArea.inputEl.style.cssText = `
			width: 100%;
			min-height: 80px;
			resize: vertical;
		`;
    const buttonContainer = this.inputContainer.createDiv();
    buttonContainer.style.cssText = "display: flex; justify-content: flex-end;";
    this.sendButton = new import_obsidian3.ButtonComponent(buttonContainer);
    this.sendButton.setButtonText("Send");
    this.sendButton.setCta();
    this.sendButton.onClick(() => this.handleSend());
    this.textArea.inputEl.addEventListener("keydown", (event) => {
      if (event.key === "Enter" && !event.shiftKey) {
        event.preventDefault();
        this.handleSend();
      }
    });
  }
  addMessage(role, content) {
    const messageEl = this.chatContainer.createDiv({ cls: `nova-message nova-message-${role}` });
    messageEl.style.cssText = `
			margin-bottom: 10px;
			padding: 8px 12px;
			border-radius: 8px;
			max-width: 85%;
			${role === "user" ? "margin-left: auto; background: var(--interactive-accent); color: var(--text-on-accent);" : "background: var(--background-primary); border: 1px solid var(--background-modifier-border);"}
		`;
    const roleEl = messageEl.createEl("div", {
      text: role === "user" ? "You" : "Nova",
      cls: "nova-message-role"
    });
    roleEl.style.cssText = `
			font-size: 0.8em;
			opacity: 0.7;
			margin-bottom: 4px;
			font-weight: 600;
		`;
    const contentEl = messageEl.createEl("div", { cls: "nova-message-content" });
    contentEl.textContent = content;
    this.chatContainer.scrollTop = this.chatContainer.scrollHeight;
  }
  async handleSend() {
    const message = this.textArea.getValue().trim();
    if (!message) return;
    this.addMessage("user", message);
    this.textArea.setValue("");
    this.sendButton.setDisabled(true);
    try {
      const loadingEl = this.chatContainer.createDiv({ cls: "nova-loading" });
      loadingEl.style.cssText = `
				padding: 8px 12px;
				background: var(--background-primary);
				border: 1px solid var(--background-modifier-border);
				border-radius: 8px;
				margin-bottom: 10px;
				max-width: 85%;
			`;
      loadingEl.textContent = "Nova is thinking...";
      const response = await this.plugin.aiProviderManager.generateText(message);
      loadingEl.remove();
      this.addMessage("assistant", response);
    } catch (error) {
      const loadingEl = this.chatContainer.querySelector(".nova-loading");
      if (loadingEl) loadingEl.remove();
      this.addMessage("assistant", `Sorry, I encountered an error: ${error.message}`);
    } finally {
      this.sendButton.setDisabled(false);
    }
  }
  async insertTextIntoActiveNote(text) {
    const activeView = this.app.workspace.getActiveViewOfType(import_obsidian3.ItemView);
    if (activeView && "editor" in activeView) {
      const editor = activeView.editor;
      if (editor) {
        const cursor = editor.getCursor();
        editor.replaceRange(text, cursor);
      }
    }
  }
};

// main.ts
var NovaPlugin = class extends import_obsidian4.Plugin {
  async onload() {
    await this.loadSettings();
    this.aiProviderManager = new AIProviderManager(this.settings);
    await this.aiProviderManager.initialize();
    this.registerView(
      VIEW_TYPE_NOVA_SIDEBAR,
      (leaf) => new NovaSidebarView(leaf, this)
    );
    this.addRibbonIcon("brain", "Nova AI", (evt) => {
      this.activateView();
    });
    this.addCommand({
      id: "open-nova-sidebar",
      name: "Open Nova sidebar",
      callback: () => {
        this.activateView();
      }
    });
    this.addSettingTab(new NovaSettingTab(this.app, this));
  }
  onunload() {
    var _a;
    (_a = this.aiProviderManager) == null ? void 0 : _a.cleanup();
  }
  async loadSettings() {
    this.settings = Object.assign({}, DEFAULT_SETTINGS, await this.loadData());
  }
  async saveSettings() {
    var _a;
    await this.saveData(this.settings);
    (_a = this.aiProviderManager) == null ? void 0 : _a.updateSettings(this.settings);
  }
  async activateView() {
    const { workspace } = this.app;
    let leaf;
    const leaves = workspace.getLeavesOfType(VIEW_TYPE_NOVA_SIDEBAR);
    if (leaves.length > 0) {
      leaf = leaves[0];
    } else {
      leaf = workspace.getRightLeaf(false);
      await (leaf == null ? void 0 : leaf.setViewState({ type: VIEW_TYPE_NOVA_SIDEBAR, active: true }));
    }
    workspace.revealLeaf(leaf);
  }
};
